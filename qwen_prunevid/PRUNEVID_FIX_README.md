# PruneVid Qwen2.5-VL å®Œæ•´å®ç°è¯´æ˜

## ç‰ˆæœ¬æ›´æ–°

**v2.0** - å®Œæ•´å®ç°PruneVidä¸‰é˜¶æ®µæ–¹æ³•ï¼š
- âœ… **Stage 1**: Spatial-Temporal Token Mergingï¼ˆVision Encoderå±‚é¢ï¼‰
- âœ… **Stage 2**: Attention-Based Token Pruningï¼ˆLLMå±‚é¢ï¼‰
- âœ… **Stage 3**: KV Cache Compressionï¼ˆLLMå±‚é¢ï¼‰

**v1.0** - ä»…å®ç°Stage 2å’ŒStage 3

## å®ç°æ¦‚è¿°

PruneVidæ˜¯ä¸€ä¸ªä¸‰é˜¶æ®µçš„è§†è§‰tokenå‰ªææ–¹æ³•ï¼š

### Stage 1: Spatial-Temporal Token Mergingï¼ˆæ–°å¢ï¼‰

åœ¨vision encoderè¾“å‡ºåã€è¿›å…¥LLMå‰è¿›è¡Œï¼š

1. **Temporal Segmentation**: ä½¿ç”¨DPC-KNNç®—æ³•å°†è§†é¢‘å¸§èšç±»æˆtemporal segments
2. **Static/Dynamic Separation**: åŸºäºå¸§é—´ç›¸ä¼¼åº¦åŒºåˆ†é™æ€å’ŒåŠ¨æ€åŒºåŸŸ
   - é™æ€åŒºåŸŸï¼šè·¨æ—¶é—´å¹³å‡ï¼Œç„¶åç©ºé—´èšç±»
   - åŠ¨æ€åŒºåŸŸï¼šæ¯å¸§ç‹¬ç«‹ä¿ç•™ï¼Œæ¯å¸§å†…ç©ºé—´èšç±»
3. **Spatial Clustering**: ä½¿ç”¨DPC-KNNå¯¹spatial tokensè¿›è¡Œèšç±»å‹ç¼©

**å‚æ•°**ï¼š
- `tau` (0.7-0.9): é™æ€/åŠ¨æ€é˜ˆå€¼ï¼Œç›¸ä¼¼åº¦>tauä¸ºé™æ€
- `cluster_ratio` (0.3-0.8): ç©ºé—´èšç±»åä¿ç•™çš„tokenæ¯”ä¾‹
- `temporal_segment_ratio` (0.25-0.5): æ—¶åºåˆ†æ®µæ¯”ä¾‹

### Stage 2: Attention-Based Token Pruningï¼ˆå·²æœ‰ï¼‰

åœ¨LLMçš„ç¬¬Nå±‚ï¼ˆé»˜è®¤ç¬¬10å±‚ï¼‰ï¼š

1. æå–text-to-visual attentionæƒé‡
2. è®¡ç®—visual tokené‡è¦æ€§ï¼š`max(text_dim) â†’ max(heads_dim)`
3. é€‰æ‹©top-ké‡è¦çš„visual tokens
4. é…ç½®custom cacheè¿›è¡Œpruning

**å‚æ•°**ï¼š
- `keep_ratio` (0.3-0.6): ä¿ç•™çš„visual tokenæ¯”ä¾‹
- `pruning_layer` (8-12): åœ¨å“ªä¸€å±‚è¿›è¡Œpruning

### Stage 3: KV Cache Compressionï¼ˆå·²æœ‰ï¼‰

åœ¨pruning_layerå®Œæˆæ—¶è‡ªåŠ¨è§¦å‘ï¼š

1. æ ¹æ®Stage 2è®¡ç®—çš„kept_indices
2. å‹ç¼©æ‰€æœ‰å±‚ï¼ˆ0åˆ°pruning_layerï¼‰çš„KV cache
3. çœŸæ­£åˆ é™¤ä¸é‡è¦çš„tokensï¼ˆseq_lenç»´åº¦å‡å°ï¼‰

---

## v1.0 ä¿®å¤çš„Bug

### ğŸ”´ åŸå®ç°çš„é—®é¢˜

1. **æœ€ä¸¥é‡**ï¼šåªæ˜¯maskç½®é›¶ï¼Œæ²¡æœ‰çœŸæ­£åˆ é™¤token
   - KV cacheä»å­˜å‚¨æ‰€æœ‰tokenï¼ˆæ— å†…å­˜èŠ‚çœï¼‰
   - Attentionä»è®¡ç®—æ‰€æœ‰tokenï¼ˆæ— è®¡ç®—åŠ é€Ÿï¼‰
2. Attentioné‡è¦æ€§è®¡ç®—ä¸ç¬¦åˆè®ºæ–‡ï¼ˆmax+meanè€Œémax+maxï¼‰
3. KV cacheå‹ç¼©é€»è¾‘å®Œå…¨æœªå®ç°

### âœ… ä¿®å¤åçš„å®ç°

1. **çœŸæ­£åˆ é™¤token**ï¼šä½¿ç”¨ç´¢å¼•é€‰æ‹©ï¼Œseq_lenç»´åº¦çœŸæ­£å‡å°
2. **è‡ªåŠ¨å‹ç¼©KV cache**ï¼šåœ¨pruning_layerå®Œæˆæ—¶è‡ªåŠ¨å‹ç¼©æ‰€æœ‰å±‚
3. **ç¬¦åˆè®ºæ–‡**ï¼šæŒ‰ç…§PLLaVAå®ç°å’Œè®ºæ–‡è¦æ±‚çš„max+maxèšåˆ

## æ–‡ä»¶å˜æ›´

### v2.0 æ–°å¢æ–‡ä»¶
- `qwen_prunevid/stage1_utils.py` - Stage 1çš„clusteringç®—æ³•ï¼ˆDPC-KNNç­‰ï¼‰
- `qwen_prunevid/stage1_wrapper.py` - Stage 1 wrapperç±»
- `test_prunevid_stage1.py` - Stage 1å‚æ•°æ‰«ææµ‹è¯•è„šæœ¬

### v1.0 æ–°å¢æ–‡ä»¶
- `qwen_prunevid/prunevid_cache.py` - PruneVidDynamicCacheç±»ï¼ˆStage 3æ ¸å¿ƒå®ç°ï¼‰

### ä¿®æ”¹æ–‡ä»¶
- `qwen_prunevid/model_wrapper.py` - é›†æˆStage 1ï¼Œä½¿ç”¨è‡ªå®šä¹‰cache
- `qwen_prunevid/qwen25_adapter.py` - ç®€åŒ–hookï¼Œé›†æˆè‡ªå®šä¹‰cache
- `qwen_prunevid/__init__.py` - å¯¼å‡ºStage 1ç›¸å…³ç±»
- `test_prunevid_fix.py` - æ·»åŠ è¶…å‚æ•°é…ç½®åŒºï¼Œæ”¯æŒStage 1æµ‹è¯•

## æ ¸å¿ƒå®ç°åŸç†

### 1. PruneVidDynamicCache

ç»§æ‰¿è‡ª`DynamicCache`ï¼Œoverride `update()`æ–¹æ³•ï¼š

```python
def update(self, key_states, value_states, layer_idx, cache_kwargs):
    # æ­£å¸¸update
    keys, values = super().update(...)

    # åœ¨pruning_layeræ—¶è§¦å‘å‹ç¼©
    if layer_idx == self.pruning_layer:
        # æ„å»ºkept_indices
        all_kept_indices = torch.cat([kept_visual_abs, text_indices])

        # å‹ç¼©æ‰€æœ‰å·²å®Œæˆå±‚çš„KV cacheï¼ˆçœŸæ­£åˆ é™¤tokenï¼‰
        for lid in range(layer_idx + 1):
            self.layers[lid].keys = old_keys[:, :, all_kept_indices, :].contiguous()
            self.layers[lid].values = old_values[:, :, all_kept_indices, :].contiguous()

    return keys, values
```

**å…³é”®ç‚¹**ï¼š
- `tensor[:, :, indices, :]` åˆ›å»ºæ–°çš„æ›´å°çš„tensorï¼ˆçœŸæ­£å‡å°seq_lenï¼‰
- å‹ç¼©æ‰€æœ‰å±‚ï¼ˆ0åˆ°pruning_layerï¼‰ï¼Œåç»­å±‚è‡ªåŠ¨çœ‹åˆ°å‹ç¼©åçš„cache

### 2. ç®€åŒ–çš„Hooké€»è¾‘

åªéœ€ä¸€ä¸ªhookè®¡ç®—importanceå’Œindicesï¼š

```python
def compute_kept_indices_hook(module, input, output):
    # 1. æå–attention weights
    attention_weights = output[1]

    # 2. è®¡ç®—importanceï¼ˆmax+maxï¼Œç¬¦åˆè®ºæ–‡ï¼‰
    text_to_visual = attention_weights[:, :, text_start:, visual_start:visual_end]
    importance = text_to_visual.max(dim=2)[0].max(dim=1)[0]

    # 3. Top-ké€‰æ‹©
    _, topk_indices = torch.topk(importance, k=num_keep)

    # 4. é…ç½®cache
    self.custom_cache.configure_pruning(
        pruning_layer=layer_idx,
        kept_visual_indices=topk_indices,
        visual_start=visual_start,
        visual_end=visual_end
    )
```

### 3. é›†æˆåˆ°generate()

```python
# åˆ›å»ºè‡ªå®šä¹‰cache
past_key_values = self.adapter.create_custom_cache()

# Generateæ—¶ä¼ å…¥
output = self.model.generate(
    **inputs,
    past_key_values=past_key_values,  # è‡ªå®šä¹‰cache
    output_attentions=True,  # å¿…é¡»ï¼
    ...
)
```

## ä½¿ç”¨æ–¹æ³•

### å®Œæ•´ä¸‰é˜¶æ®µPruneVidï¼ˆæ¨èï¼‰

```python
from qwen_prunevid import Qwen25VLPruneVid

# åˆ›å»ºmodelï¼ˆå¯ç”¨Stage 1 + Stage 2ï¼‰
model = Qwen25VLPruneVid(
    model_path="Qwen/Qwen2.5-VL-7B-Instruct",
    # Stage 1å‚æ•°
    enable_stage1=True,
    tau=0.8,
    cluster_ratio=0.5,
    temporal_segment_ratio=0.25,
    # Stage 2å‚æ•°
    enable_pruning=True,
    keep_ratio=0.4,
    pruning_layer=10,
    verbose=True
)

# ç”Ÿæˆ
result = model.generate(
    video_path="video.mp4",
    question="What is happening?",
    max_new_tokens=100
)

print(f"ç­”æ¡ˆ: {result['generated_text']}")
print(f"Stage 1: {result['tokens_before_stage1']} â†’ {result['tokens_after_stage1']}")
print(f"Stage 2: {result['tokens_before']} â†’ {result['tokens_after']}")
print(f"æ€»å‹ç¼©æ¯”: {result['total_compression_ratio']:.1%}")
```

### ä»…Stage 2ï¼ˆv1.0æ–¹å¼ï¼‰

```python
# ä»…ä½¿ç”¨Stage 2 + Stage 3
model = Qwen25VLPruneVid(
    model_path="Qwen/Qwen2.5-VL-7B-Instruct",
    enable_stage1=False,  # å…³é—­Stage 1
    enable_pruning=True,
    keep_ratio=0.4,
    pruning_layer=10,
)

result = model.generate(
    video_path="video.mp4",
    question="What is happening?"
)
```

### ä»…Stage 1

```python
# ä»…ä½¿ç”¨Stage 1ï¼ˆæµ‹è¯•vision encoderå±‚é¢çš„å‹ç¼©ï¼‰
model = Qwen25VLPruneVid(
    model_path="Qwen/Qwen2.5-VL-7B-Instruct",
    enable_stage1=True,
    tau=0.8,
    cluster_ratio=0.5,
    temporal_segment_ratio=0.25,
    enable_pruning=False,  # å…³é—­Stage 2
)
```

### éªŒè¯æ•ˆæœ

#### åŸºç¡€æµ‹è¯•

```bash
# æµ‹è¯•å½“å‰é…ç½®
python test_prunevid_fix.py video.mp4 "What is happening in the video?"
```

åœ¨`test_prunevid_fix.py`é¡¶éƒ¨ä¿®æ”¹`CURRENT_PRESET`åˆ‡æ¢é…ç½®ï¼š
- `'baseline'`: æ— pruning
- `'stage2_only'`: ä»…Stage 2
- `'stage1_only'`: ä»…Stage 1
- `'default'`: Stage 1 + Stage 2ï¼ˆé»˜è®¤å‚æ•°ï¼‰
- `'conservative'`: ä¿å®ˆé…ç½®ï¼ˆä¼˜å…ˆç²¾åº¦ï¼‰
- `'aggressive'`: æ¿€è¿›é…ç½®ï¼ˆæœ€å¤§å‹ç¼©ï¼‰
- `'custom'`: è‡ªå®šä¹‰å‚æ•°

#### Stage 1å‚æ•°æ‰«æ

```bash
# è‡ªåŠ¨æµ‹è¯•å¤šç§Stage 1å‚æ•°ç»„åˆ
python test_prunevid_stage1.py video.mp4 "What is happening?"
```

ä¼šæµ‹è¯•ï¼š
- tau: [0.7, 0.8, 0.9]
- cluster_ratio: [0.3, 0.5, 0.7]
- temporal_segment_ratio: [0.25, 0.5]

ç»“æœä¿å­˜åˆ°`stage1_sweep_results.json`

### é¢„æœŸæ•ˆæœ

#### å®Œæ•´ä¸‰é˜¶æ®µï¼ˆStage 1 + Stage 2ï¼‰

| é…ç½® | Stage 1å‹ç¼© | Stage 2å‹ç¼© | æ€»å‹ç¼© | é¢„æœŸåŠ é€Ÿ | ç²¾åº¦å½±å“ |
|------|------------|------------|--------|---------|---------|
| Conservative | ~20% | ~40% | ~52% | 1.5x | <2% |
| Default | ~50% | ~60% | ~80% | 2-3x | 2-3% |
| Aggressive | ~50% | ~70% | ~85% | 3-4x | 3-5% |

#### ä»…Stage 2ï¼ˆv1.0ï¼‰

| æŒ‡æ ‡ | é¢„æœŸï¼ˆkeep_ratio=0.4ï¼‰ |
|------|----------------------|
| Tokenä¿ç•™ç‡ | ~40% |
| å‹ç¼©æ¯” | ~60% |
| FLOPså‡å°‘ | 50-60% |
| TTFTåŠ é€Ÿ | 1.3-1.5x |
| å†…å­˜èŠ‚çœ | 40-50% |
| å‡†ç¡®ç‡ | ä¸baselineç›¸å½“ |

## å…³é”®æ³¨æ„äº‹é¡¹

### âš ï¸ å¿…é¡»å¯ç”¨output_attentions

```python
output = model.generate(
    ...,
    output_attentions=True,  # å¿…é¡»ï¼å¦åˆ™æ— æ³•è·å–attentionæƒé‡
)
```

æ²¡æœ‰attentionæƒé‡æ—¶ä¼šfallbackåˆ°åŸºäºnormçš„importanceï¼Œæ•ˆæœå¯èƒ½ä¸ä½³ã€‚

### âš ï¸ Stage 1å‚æ•°è°ƒä¼˜

**tauï¼ˆé™æ€/åŠ¨æ€é˜ˆå€¼ï¼‰**ï¼š
- `tau=0.9`ï¼šä¸¥æ ¼ï¼Œæ›´å°‘é™æ€åŒºåŸŸï¼Œä¿ç•™æ›´å¤šåŠ¨æ€ä¿¡æ¯
- `tau=0.8`ï¼šé»˜è®¤ï¼Œå¹³è¡¡
- `tau=0.7`ï¼šå®½æ¾ï¼Œæ›´å¤šé™æ€åŒºåŸŸï¼Œæ›´é«˜å‹ç¼©

**cluster_ratioï¼ˆç©ºé—´èšç±»æ¯”ä¾‹ï¼‰**ï¼š
- `cluster_ratio=0.8`ï¼šä¿å®ˆï¼Œæ›´å°‘å‹ç¼©
- `cluster_ratio=0.5`ï¼šé»˜è®¤ï¼Œå¹³è¡¡
- `cluster_ratio=0.3`ï¼šæ¿€è¿›ï¼Œæœ€å¤§å‹ç¼©

**temporal_segment_ratioï¼ˆæ—¶åºåˆ†æ®µæ¯”ä¾‹ï¼‰**ï¼š
- `temporal_segment_ratio=0.5`ï¼šæ›´å°‘segmentsï¼Œæ¯ä¸ªæ›´é•¿
- `temporal_segment_ratio=0.25`ï¼šé»˜è®¤ï¼Œå¹³è¡¡
- `temporal_segment_ratio=0.1`ï¼šæ›´å¤šsegmentsï¼Œæ›´ç²¾ç»†çš„æ—¶åºå»ºæ¨¡

**è°ƒä¼˜ç­–ç•¥**ï¼š
1. å…ˆè¿è¡Œ`test_prunevid_stage1.py`è¿›è¡Œå‚æ•°æ‰«æ
2. æ ¹æ®ä½ çš„éœ€æ±‚é€‰æ‹©åˆé€‚é…ç½®ï¼š
   - ä¼˜å…ˆç²¾åº¦ï¼šæé«˜æ‰€æœ‰å‚æ•°ï¼ˆ0.9, 0.8, 0.5ï¼‰
   - å¹³è¡¡ï¼šé»˜è®¤å‚æ•°ï¼ˆ0.8, 0.5, 0.25ï¼‰
   - ä¼˜å…ˆæ•ˆç‡ï¼šé™ä½å‚æ•°ï¼ˆ0.7, 0.3, 0.25ï¼‰

### âš ï¸ Stage 2å‚æ•°è°ƒä¼˜

**keep_ratioï¼ˆä¿ç•™æ¯”ä¾‹ï¼‰**ï¼š
- `keep_ratio=0.6`ï¼šä¿å®ˆï¼Œæ›´å¥½çš„å‡†ç¡®ç‡
- `keep_ratio=0.4`ï¼šé»˜è®¤ï¼Œå¹³è¡¡
- `keep_ratio=0.3`ï¼šæ¿€è¿›ï¼Œæœ€å¤§å‹ç¼©

**pruning_layerï¼ˆå‰ªæå±‚ï¼‰**ï¼š
- `pruning_layer=10`ï¼šé»˜è®¤ï¼ˆQwen2.5-7Bå…±32å±‚ï¼‰
- å¤ªæ—©ï¼ˆ<8ï¼‰ï¼šattentionå¯èƒ½ä¸ç¨³å®š
- å¤ªæ™šï¼ˆ>15ï¼‰ï¼šèŠ‚çœçš„è®¡ç®—é‡å‡å°‘

## è°ƒè¯•æŠ€å·§

### æŸ¥çœ‹è¯¦ç»†æ—¥å¿—

```python
model = Qwen25VLPruneVid(..., verbose=True)
```

ä¼šæ‰“å°ï¼š
- Visual tokenæ£€æµ‹ä¿¡æ¯
- Attention shape
- Importanceè®¡ç®—æ–¹å¼
- Pruningç»Ÿè®¡

### æ£€æŸ¥cache shape

åœ¨hookä¸­æ·»åŠ ï¼š

```python
print(f"Before pruning: {cache.key_cache[0].shape}")
# åº”è¯¥åœ¨pruningåå˜å°
```

### å¯¹æ¯”baseline

```python
# Baseline
model_base = Qwen25VLPruneVid(..., enable_pruning=False)
result_base = model_base.generate(...)

# PruneVid
model_prune = Qwen25VLPruneVid(..., enable_pruning=True)
result_prune = model_prune.generate(...)

# å¯¹æ¯”æ—¶é—´ã€å†…å­˜ã€å‡†ç¡®ç‡
```

## æŠ€æœ¯ç»†èŠ‚

### ä¸ºä»€ä¹ˆä½¿ç”¨è‡ªå®šä¹‰Cacheè€Œä¸æ˜¯Hookï¼Ÿ

**Hookæ–¹æ¡ˆçš„é—®é¢˜**ï¼š
- éš¾ä»¥åœ¨hookä¸­åŒæ—¶ä¿®æ”¹hidden_statesã€past_key_valuesã€attention_maskç­‰
- éœ€è¦åè°ƒå¤šä¸ªhookï¼Œé€»è¾‘å¤æ‚
- å¯èƒ½ä¸transformerså†…éƒ¨é€»è¾‘å†²çª

**è‡ªå®šä¹‰Cacheçš„ä¼˜åŠ¿**ï¼š
- åœ¨Cacheå±‚é¢ç»Ÿä¸€å¤„ç†ï¼Œé€»è¾‘æ¸…æ™°
- è‡ªåŠ¨propagateåˆ°åç»­å±‚
- ä¸transformersæ¶æ„å…¼å®¹æ€§å¥½

### çœŸæ­£åˆ é™¤ vs Maskç½®é›¶

```python
# âŒ é”™è¯¯ï¼šMaskç½®é›¶
mask = torch.zeros(seq_len)
mask[kept_indices] = 1.0
hidden_states = hidden_states * mask  # åºåˆ—é•¿åº¦ä»æ˜¯seq_len

# âœ… æ­£ç¡®ï¼šç´¢å¼•é€‰æ‹©
hidden_states = hidden_states[:, kept_indices, :]  # åºåˆ—é•¿åº¦å˜ä¸ºlen(kept_indices)
```

Maskç½®é›¶ï¼š
- åºåˆ—é•¿åº¦ä¸å˜
- Attentionä»è®¡ç®—æ‰€æœ‰ä½ç½®ï¼ˆåŒ…æ‹¬0ï¼‰
- æ— åŠ é€Ÿã€æ— å†…å­˜èŠ‚çœ

ç´¢å¼•é€‰æ‹©ï¼š
- åºåˆ—é•¿åº¦çœŸæ­£å‡å°
- Attentionåªè®¡ç®—ä¿ç•™çš„token
- å®é™…åŠ é€Ÿå’Œå†…å­˜èŠ‚çœ

## å¸¸è§é—®é¢˜

### Q: ä¸ºä»€ä¹ˆæˆ‘çœ‹ä¸åˆ°åŠ é€Ÿï¼Ÿ

æ£€æŸ¥ï¼š
1. `output_attentions=True` æ˜¯å¦å¯ç”¨
2. Verboseæ—¥å¿—ä¸­pruningæ˜¯å¦çœŸçš„åº”ç”¨äº†
3. Cache shapeæ˜¯å¦çœŸçš„å˜å°äº†

### Q: å‡†ç¡®ç‡ä¸‹é™æ€ä¹ˆåŠï¼Ÿ

è°ƒä¼˜ï¼š
1. å¢å¤§keep_ratioï¼ˆå¦‚ä»0.4åˆ°0.5ï¼‰
2. è°ƒæ•´pruning_layerï¼ˆå¦‚ä»10åˆ°12ï¼‰
3. ç¡®è®¤attentionæƒé‡æ­£ç¡®è·å–ï¼ˆä¸æ˜¯fallbackåˆ°normï¼‰

### Q: å¦‚ä½•åœ¨EgoSchemaä¸Šæµ‹è¯•ï¼Ÿ

å‚è€ƒ `eval_qwen25_prunevid_egoschema.py`ï¼š

```python
model = Qwen25VLPruneVid(
    model_path="Qwen/Qwen2.5-VL-7B-Instruct",
    enable_pruning=True,
    keep_ratio=0.4,
    verbose=False  # è¯„ä¼°æ—¶å…³é—­verbose
)

# å¤„ç†æ¯ä¸ªæ ·æœ¬
prediction, text, stats = model.process_egoschema_sample(
    video_path, question, options
)
```

## å‚è€ƒèµ„æ–™

- è®ºæ–‡ï¼šPruneVid: Visual Token Pruning for Efficient Video Large Language Models (ACL 2025)
- PLLaVAå®ç°ï¼š`/mnt/ssd_ext/huggingface/prunevid/models/pllava/elastic_cache.py`
- Qwen2.5-VLæ–‡æ¡£ï¼šhttps://github.com/QwenLM/Qwen2.5-VL

## è´¡çŒ®è€…

ä¿®å¤å®ç°åŸºäºå¯¹è®ºæ–‡å’ŒPLLaVAå®˜æ–¹ä»£ç çš„æ·±å…¥ç ”ç©¶ã€‚

"""
EgoSchemaè¯„ä¼°è„šæœ¬ - PruneVid for Qwen2.5-VL

åœ¨EgoSchemaæ•°æ®é›†ä¸Šè¯„ä¼°PruneVidæ–¹æ³•çš„æ€§èƒ½å’Œtokenå‹ç¼©æ•ˆæœ
"""

import os
import json
import torch
from pathlib import Path
from tqdm import tqdm
from typing import Dict, List, Optional
import time
from datasets import load_dataset

# ============================================================================
# å¯è°ƒè¶…å‚æ•°é…ç½®åŒºåŸŸ
# ============================================================================

# GPUé…ç½®
GPU_ID = 0  # ä½¿ç”¨å“ªå—GPUï¼ˆ0, 1, 2, ...ï¼‰
DEVICE = f"cuda:{GPU_ID}"

# æ•°æ®é›†é…ç½®
VIDEO_DIR = "/mnt/ssd_ext/huggingface/egoschema/videos"  # è§†é¢‘æ–‡ä»¶å¤¹
# ä½¿ç”¨datasetsåº“åŠ è½½EgoSchemaæ•°æ®é›†
DATASET_NAME = "lmms-lab/egoschema"
DATASET_SPLIT = "test"  # 'test' or 'validation'

# æ¨¡å‹é…ç½®
MODEL_PATH = "/mnt/ssd_ext/huggingface/models/Qwen2.5-VL-7B-Instruct"  # Qwen2.5-VLæ¨¡å‹è·¯å¾„

# Stage 1: æ—¶ç©ºTokenåˆå¹¶
ENABLE_STAGE1 = True
TAU = 0.8  # é™æ€/åŠ¨æ€åˆ†ç¦»é˜ˆå€¼ (0.6-0.9)
CLUSTER_RATIO = 0.5  # ç©ºé—´èšç±»ä¿ç•™æ¯”ä¾‹ (0.3-0.7)
TEMPORAL_SEGMENT_RATIO = 0.25  # æ—¶åºåˆ†æ®µæ¯”ä¾‹ (0.125-0.5)
DPC_KNN_K = 5  # DPC-KNNçš„kè¿‘é‚»å‚æ•°

# Stage 2: åŸºäºæ³¨æ„åŠ›çš„Tokené€‰æ‹©
# æ³¨æ„ï¼šæš‚æ—¶ç¦ç”¨ Stage 2ï¼Œä¸“æ³¨æµ‹è¯• Stage 1
ENABLE_STAGE2 = False  # æ”¹ä¸º Falseï¼Œåªæµ‹è¯• Stage 1
KEEP_RATIO = 0.5  # Tokenä¿ç•™æ¯”ä¾‹ (0.2-0.6)
PRUNING_LAYER = 10  # åœ¨å“ªä¸€å±‚è¿›è¡Œå‰ªæ (5-15)
ATTENTION_AGGREGATION = "max"  # 'max' or 'mean'

# è§†é¢‘å¤„ç†é…ç½®
MAX_FRAMES = 16  # æœ€å¤§å¸§æ•° (8, 16, 32)
MIN_PIXELS = 224 * 224
# æ³¨æ„ï¼šQwen2.5-VLåœ¨å¤„ç†è§†é¢‘æ—¶æœ‰è¿è¡Œæ—¶é™åˆ¶602112 (çº¦776x776)
# è™½ç„¶é…ç½®æ–‡ä»¶ä¸­max_pixels=12845056ï¼Œä½†å®é™…å¤„ç†è§†é¢‘æ—¶ä¼šæœ‰æ›´ä¸¥æ ¼çš„é™åˆ¶
# å¯¹äº16å¸§: ä½¿ç”¨ 192x192æ¯å¸§ï¼Œæ€»å…± 589824 pixels (åœ¨602112é™åˆ¶å†…)
MAX_PIXELS = 192 * 192 * MAX_FRAMES  # 589824 < 602112

# ç”Ÿæˆé…ç½®
MAX_NEW_TOKENS = 10  # EgoSchemaæ˜¯é€‰æ‹©é¢˜ï¼Œç­”æ¡ˆå¾ˆçŸ­
TEMPERATURE = 0.0  # 0è¡¨ç¤ºgreedy decoding
DO_SAMPLE = False

# æµ‹è¯•é…ç½®
NUM_SAMPLES = 10  # æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
START_INDEX = 0  # ä»ç¬¬å‡ ä¸ªæ ·æœ¬å¼€å§‹
SAVE_RESULTS = True  # æ˜¯å¦ä¿å­˜ç»“æœ
OUTPUT_DIR = "./results"  # ç»“æœä¿å­˜ç›®å½•
VERBOSE = True  # æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

# ============================================================================
# ä»£ç å¼€å§‹
# ============================================================================

# è®¾ç½®CUDAè®¾å¤‡
os.environ["CUDA_VISIBLE_DEVICES"] = str(GPU_ID)

import sys
sys.path.insert(0, str(Path(__file__).parent))

from config import PruneVidConfig
from model_wrapper import Qwen25VLPruneVid


class EgoSchemaEvaluator:
    """EgoSchemaè¯„ä¼°å™¨"""

    def __init__(
        self,
        model: Qwen25VLPruneVid,
        video_dir: str,
        dataset_name: str = "lmms-lab/egoschema",
        dataset_split: str = "test",
    ):
        self.model = model
        self.video_dir = Path(video_dir)
        self.dataset_name = dataset_name
        self.dataset_split = dataset_split

        # åŠ è½½æ•°æ®é›†
        self.load_dataset()

        # ç»Ÿè®¡ä¿¡æ¯
        self.total_samples = 0
        self.correct_samples = 0
        self.total_tokens_before = 0
        self.total_tokens_after_stage1 = 0
        self.total_tokens_after_stage2 = 0
        self.results = []

    def load_dataset(self):
        """åŠ è½½EgoSchemaæ•°æ®é›†"""
        print(f"Loading dataset {self.dataset_name} ({self.dataset_split})...")
        self.dataset = load_dataset(self.dataset_name, "Subset", split=self.dataset_split)
        print(f"Loaded {len(self.dataset)} questions")

    def format_question(self, sample: Dict) -> str:
        """
        æ ¼å¼åŒ–é—®é¢˜ä¸ºé€‰æ‹©é¢˜å½¢å¼

        Args:
            sample: æ ‡æ³¨æ ·æœ¬

        Returns:
            formatted_question: æ ¼å¼åŒ–åçš„é—®é¢˜
        """
        question = sample['question']
        options = sample['option']  # options is already a list

        formatted = f"{question}\n\n"
        formatted += "Options:\n"
        for i, opt in enumerate(options):
            formatted += f"{i}. {opt}\n"
        formatted += "\nAnswer with the option number (0-4):"

        return formatted

    def extract_answer(self, generated_text: str) -> Optional[int]:
        """
        ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–ç­”æ¡ˆé€‰é¡¹

        Args:
            generated_text: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬

        Returns:
            answer: ç­”æ¡ˆé€‰é¡¹ (0-4)ï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å›None
        """
        # æ¸…ç†æ–‡æœ¬
        text = generated_text.strip().lower()

        # å°è¯•å¤šç§æå–æ–¹å¼
        # 1. ç›´æ¥æ˜¯æ•°å­—
        if text in ['0', '1', '2', '3', '4']:
            return int(text)

        # 2. å­—æ¯å½¢å¼ (A=0, B=1, C=2, D=3, E=4)
        if text.startswith('a'):
            return 0
        elif text.startswith('b'):
            return 1
        elif text.startswith('c'):
            return 2
        elif text.startswith('d'):
            return 3
        elif text.startswith('e'):
            return 4

        # 3. åŒ…å«"option X"æˆ–"é€‰é¡¹ X"
        for i in range(5):
            if f"option {i}" in text or f"é€‰é¡¹ {i}" in text or f"option{i}" in text:
                return i

        # 4. æŸ¥æ‰¾å­—æ¯é€‰é¡¹
        letter_map = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4}
        for char in text:
            if char in letter_map:
                return letter_map[char]

        # 5. æŸ¥æ‰¾æ•°å­—0-4
        for char in text:
            if char in '01234':
                return int(char)

        # 6. æ— æ³•æå–
        return None

    def evaluate_sample(self, sample: Dict, sample_idx: int) -> Dict:
        """
        è¯„ä¼°å•ä¸ªæ ·æœ¬

        Args:
            sample: æ ‡æ³¨æ ·æœ¬
            sample_idx: æ ·æœ¬ç´¢å¼•

        Returns:
            result: è¯„ä¼°ç»“æœ
        """
        # è·å–è§†é¢‘è·¯å¾„
        video_id = sample['video_idx']
        video_path = self.video_dir / f"{video_id}.mp4"

        if not video_path.exists():
            print(f"Warning: Video not found: {video_path}")
            return None

        # æ ¼å¼åŒ–é—®é¢˜
        question = self.format_question(sample)

        # æ¨¡å‹æ¨ç†
        try:
            result = self.model.generate(
                video_path=str(video_path),
                question=question,
                max_new_tokens=MAX_NEW_TOKENS,
                temperature=TEMPERATURE,
                do_sample=DO_SAMPLE,
                return_dict=True,
            )

            generated_text = result['generated_text']
            compression_stats = result['compression_stats']

            # æå–ç­”æ¡ˆ
            predicted_answer = self.extract_answer(generated_text)
            ground_truth = int(sample['answer'])

            # åˆ¤æ–­æ­£ç¡®æ€§
            is_correct = (predicted_answer == ground_truth)

            # è·å–tokenç»Ÿè®¡
            tokens_before = compression_stats['tokens_before']
            tokens_after = compression_stats['tokens_after']

            # è·å–å„é˜¶æ®µçš„tokenæ•°é‡
            detailed_stats = compression_stats.get('detailed_stats', {})
            stage1_stats = detailed_stats.get('stage1', {})
            stage3_stats = detailed_stats.get('stage3', {})

            tokens_after_stage1 = stage1_stats.get('tokens_after', tokens_before) if stage1_stats.get('enabled', False) else tokens_before
            tokens_after_stage2 = stage3_stats.get('kept_visual_tokens', tokens_after_stage1) if stage3_stats.get('compressed', False) else tokens_after_stage1

            # è¿”å›ç»“æœ
            return {
                'sample_idx': sample_idx,
                'video_id': video_id,
                'question': sample['question'],
                'ground_truth': ground_truth,
                'predicted_answer': predicted_answer,
                'generated_text': generated_text,
                'is_correct': is_correct,
                'tokens_before': tokens_before,
                'tokens_after_stage1': tokens_after_stage1,
                'tokens_after_stage2': tokens_after_stage2,
                'tokens_after': tokens_after,
                'compression_stats': compression_stats,
            }

        except Exception as e:
            print(f"Error processing sample {sample_idx}: {e}")
            import traceback
            traceback.print_exc()
            return None

    def print_sample_result(self, result: Dict):
        """æ‰“å°å•ä¸ªæ ·æœ¬çš„ç»“æœ"""
        print(f"\n{'='*80}")
        print(f"Sample {result['sample_idx'] + 1}/{self.total_samples}")
        print(f"Video ID: {result['video_id']}")
        print(f"Question: {result['question'][:100]}...")
        print(f"\nGround Truth: {result['ground_truth']}")
        print(f"Predicted:    {result['predicted_answer']}")
        print(f"Generated:    {result['generated_text'][:100]}...")
        print(f"Correct: {'âœ“' if result['is_correct'] else 'âœ—'}")

        # å½“å‰ç´¯è®¡å‡†ç¡®ç‡
        accuracy = self.correct_samples / self.total_samples * 100
        print(f"\nğŸ“Š Current Accuracy: {self.correct_samples}/{self.total_samples} = {accuracy:.2f}%")

        # Tokenå‹ç¼©ç»Ÿè®¡
        tokens_before = result['tokens_before']
        tokens_after_stage1 = result['tokens_after_stage1']
        tokens_after_stage2 = result['tokens_after_stage2']

        stage1_drop = (tokens_before - tokens_after_stage1) / tokens_before * 100 if tokens_before > 0 else 0
        stage2_drop = (tokens_after_stage1 - tokens_after_stage2) / tokens_after_stage1 * 100 if tokens_after_stage1 > 0 else 0
        total_drop = (tokens_before - tokens_after_stage2) / tokens_before * 100 if tokens_before > 0 else 0

        print(f"\nğŸ“‰ Token Compression (Current Sample):")
        print(f"  Original:      {tokens_before}")
        print(f"  After Stage 1: {tokens_after_stage1} (drop: {stage1_drop:.1f}%)")
        print(f"  After Stage 2: {tokens_after_stage2} (drop: {stage2_drop:.1f}%)")
        print(f"  Total drop:    {total_drop:.1f}%")

        # ç´¯è®¡å¹³å‡å‹ç¼©ç‡
        avg_stage1_drop = (self.total_tokens_before - self.total_tokens_after_stage1) / self.total_tokens_before * 100 if self.total_tokens_before > 0 else 0
        avg_stage2_drop = (self.total_tokens_after_stage1 - self.total_tokens_after_stage2) / self.total_tokens_after_stage1 * 100 if self.total_tokens_after_stage1 > 0 else 0
        avg_total_drop = (self.total_tokens_before - self.total_tokens_after_stage2) / self.total_tokens_before * 100 if self.total_tokens_before > 0 else 0

        print(f"\nğŸ“‰ Average Token Compression (Cumulative):")
        print(f"  Stage 1 avg drop: {avg_stage1_drop:.1f}%")
        print(f"  Stage 2 avg drop: {avg_stage2_drop:.1f}%")
        print(f"  Total avg drop:   {avg_total_drop:.1f}%")
        print(f"{'='*80}\n")

    def run_evaluation(
        self,
        num_samples: Optional[int] = None,
        start_index: int = 0,
    ):
        """
        è¿è¡Œè¯„ä¼°

        Args:
            num_samples: è¯„ä¼°æ ·æœ¬æ•°é‡ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
            start_index: èµ·å§‹ç´¢å¼•
        """
        # ç¡®å®šè¦è¯„ä¼°çš„æ ·æœ¬
        end_index = len(self.dataset) if num_samples is None else min(start_index + num_samples, len(self.dataset))
        samples_to_eval = self.dataset.select(range(start_index, end_index))

        print(f"\n{'='*80}")
        print(f"Starting evaluation on {len(samples_to_eval)} samples (from index {start_index})")
        print(f"{'='*80}\n")

        # è¯„ä¼°æ¯ä¸ªæ ·æœ¬
        start_time = time.time()

        for i, sample in enumerate(tqdm(samples_to_eval, desc="Evaluating")):
            sample_idx = start_index + i

            # è¯„ä¼°æ ·æœ¬
            result = self.evaluate_sample(sample, sample_idx)

            if result is None:
                continue

            # æ›´æ–°ç»Ÿè®¡
            self.total_samples += 1
            if result['is_correct']:
                self.correct_samples += 1

            self.total_tokens_before += result['tokens_before']
            self.total_tokens_after_stage1 += result['tokens_after_stage1']
            self.total_tokens_after_stage2 += result['tokens_after_stage2']

            self.results.append(result)

            # æ‰“å°ç»“æœ
            if VERBOSE:
                self.print_sample_result(result)

        total_time = time.time() - start_time

        # æ‰“å°æœ€ç»ˆç»“æœ
        self.print_final_results(total_time)

    def print_final_results(self, total_time: float):
        """æ‰“å°æœ€ç»ˆè¯„ä¼°ç»“æœ"""
        print(f"\n{'='*80}")
        print(f"ğŸ‰ EVALUATION COMPLETED")
        print(f"{'='*80}\n")

        # å‡†ç¡®ç‡
        accuracy = self.correct_samples / self.total_samples * 100 if self.total_samples > 0 else 0
        print(f"ğŸ“Š Final Accuracy:")
        print(f"  Correct: {self.correct_samples}/{self.total_samples}")
        print(f"  Accuracy: {accuracy:.2f}%")

        # Tokenå‹ç¼©ç»Ÿè®¡
        avg_stage1_drop = (self.total_tokens_before - self.total_tokens_after_stage1) / self.total_tokens_before * 100 if self.total_tokens_before > 0 else 0
        avg_stage2_drop = (self.total_tokens_after_stage1 - self.total_tokens_after_stage2) / self.total_tokens_after_stage1 * 100 if self.total_tokens_after_stage1 > 0 else 0
        avg_total_drop = (self.total_tokens_before - self.total_tokens_after_stage2) / self.total_tokens_before * 100 if self.total_tokens_before > 0 else 0

        print(f"\nğŸ“‰ Final Token Compression:")
        print(f"  Total tokens before:       {self.total_tokens_before}")
        print(f"  Total tokens after Stage 1: {self.total_tokens_after_stage1}")
        print(f"  Total tokens after Stage 2: {self.total_tokens_after_stage2}")
        print(f"\n  Stage 1 drop ratio: {avg_stage1_drop:.2f}%")
        print(f"  Stage 2 drop ratio: {avg_stage2_drop:.2f}%")
        print(f"  Total drop ratio:   {avg_total_drop:.2f}%")

        # æ—¶é—´ç»Ÿè®¡
        avg_time_per_sample = total_time / self.total_samples if self.total_samples > 0 else 0
        print(f"\nâ±ï¸  Time Statistics:")
        print(f"  Total time: {total_time:.2f}s")
        print(f"  Avg time per sample: {avg_time_per_sample:.2f}s")

        print(f"\n{'='*80}\n")

        # ä¿å­˜ç»“æœ
        if SAVE_RESULTS:
            self.save_results()

    def save_results(self):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        os.makedirs(OUTPUT_DIR, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        stage_suffix = ""
        if ENABLE_STAGE1 and ENABLE_STAGE2:
            stage_suffix = f"_s1s2_tau{TAU}_keep{KEEP_RATIO}"
        elif ENABLE_STAGE1:
            stage_suffix = f"_s1_tau{TAU}"
        elif ENABLE_STAGE2:
            stage_suffix = f"_s2_keep{KEEP_RATIO}"
        else:
            stage_suffix = "_baseline"

        filename = f"egoschema_results_{timestamp}{stage_suffix}.json"
        filepath = os.path.join(OUTPUT_DIR, filename)

        # å‡†å¤‡ä¿å­˜çš„æ•°æ®
        save_data = {
            'config': {
                'enable_stage1': ENABLE_STAGE1,
                'tau': TAU,
                'cluster_ratio': CLUSTER_RATIO,
                'temporal_segment_ratio': TEMPORAL_SEGMENT_RATIO,
                'enable_stage2': ENABLE_STAGE2,
                'keep_ratio': KEEP_RATIO,
                'pruning_layer': PRUNING_LAYER,
                'max_frames': MAX_FRAMES,
            },
            'summary': {
                'total_samples': self.total_samples,
                'correct_samples': self.correct_samples,
                'accuracy': self.correct_samples / self.total_samples * 100 if self.total_samples > 0 else 0,
                'stage1_drop_ratio': (self.total_tokens_before - self.total_tokens_after_stage1) / self.total_tokens_before * 100 if self.total_tokens_before > 0 else 0,
                'stage2_drop_ratio': (self.total_tokens_after_stage1 - self.total_tokens_after_stage2) / self.total_tokens_after_stage1 * 100 if self.total_tokens_after_stage1 > 0 else 0,
                'total_drop_ratio': (self.total_tokens_before - self.total_tokens_after_stage2) / self.total_tokens_before * 100 if self.total_tokens_before > 0 else 0,
            },
            'results': self.results,
        }

        # ä¿å­˜
        with open(filepath, 'w') as f:
            json.dump(save_data, f, indent=2)

        print(f"âœ… Results saved to: {filepath}")


def main():
    """ä¸»å‡½æ•°"""
    print(f"\n{'='*80}")
    print(f"EgoSchema Evaluation - PruneVid for Qwen2.5-VL")
    print(f"{'='*80}\n")

    # æ‰“å°é…ç½®
    print("Configuration:")
    print(f"  GPU: {GPU_ID} (device: {DEVICE})")
    print(f"  Model: {MODEL_PATH}")
    print(f"  Dataset: {DATASET_NAME} ({DATASET_SPLIT})")
    print(f"  Video dir: {VIDEO_DIR}")
    print(f"\nPruneVid Settings:")
    print(f"  Stage 1: {'ON' if ENABLE_STAGE1 else 'OFF'}")
    if ENABLE_STAGE1:
        print(f"    - tau: {TAU}")
        print(f"    - cluster_ratio: {CLUSTER_RATIO}")
        print(f"    - temporal_segment_ratio: {TEMPORAL_SEGMENT_RATIO}")
    print(f"  Stage 2: {'ON' if ENABLE_STAGE2 else 'OFF'}")
    if ENABLE_STAGE2:
        print(f"    - keep_ratio: {KEEP_RATIO}")
        print(f"    - pruning_layer: {PRUNING_LAYER}")
    print(f"\nTest Settings:")
    print(f"  Num samples: {NUM_SAMPLES if NUM_SAMPLES else 'All'}")
    print(f"  Start index: {START_INDEX}")
    print(f"  Max frames: {MAX_FRAMES}")
    print(f"\n{'='*80}\n")

    # åˆ›å»ºé…ç½®
    config = PruneVidConfig(
        enable_stage1=ENABLE_STAGE1,
        tau=TAU,
        cluster_ratio=CLUSTER_RATIO,
        temporal_segment_ratio=TEMPORAL_SEGMENT_RATIO,
        dpc_knn_k=DPC_KNN_K,
        enable_pruning=ENABLE_STAGE2,
        keep_ratio=KEEP_RATIO,
        pruning_layer=PRUNING_LAYER,
        attention_aggregation=ATTENTION_AGGREGATION,
        max_frames=MAX_FRAMES,
        min_pixels=MIN_PIXELS,
        max_pixels=MAX_PIXELS,
        max_new_tokens=MAX_NEW_TOKENS,
        temperature=TEMPERATURE,
        do_sample=DO_SAMPLE,
        device=DEVICE,
        verbose=VERBOSE,
    )

    # åŠ è½½æ¨¡å‹
    print("Loading model...")
    model = Qwen25VLPruneVid(
        model_path=MODEL_PATH,
        config=config,
        device=DEVICE,
        torch_dtype=torch.bfloat16,
    )
    print("Model loaded!\n")

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = EgoSchemaEvaluator(
        model=model,
        video_dir=VIDEO_DIR,
        dataset_name=DATASET_NAME,
        dataset_split=DATASET_SPLIT,
    )

    # è¿è¡Œè¯„ä¼°
    evaluator.run_evaluation(
        num_samples=NUM_SAMPLES,
        start_index=START_INDEX,
    )


if __name__ == "__main__":
    main()

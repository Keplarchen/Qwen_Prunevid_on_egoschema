"""
EgoSchemaè¯„ä¼°è„šæœ¬ - åŸå§‹Qwen2.5-VL-7B-Instruct (Baseline)

åœ¨EgoSchemaæ•°æ®é›†ä¸Šè¯„ä¼°åŸå§‹Qwen2.5-VLæ¨¡å‹çš„æ€§èƒ½ï¼Œä½œä¸ºbaselineå¯¹æ¯”ã€‚
ä½¿ç”¨transformersåŸç”Ÿå®ç°ï¼Œä¸åŒ…å«ä»»ä½•PruneVidä¿®æ”¹ã€‚
"""

import os
import json
import torch
import cv2
import numpy as np
from pathlib import Path
from tqdm import tqdm
from typing import Dict, List, Optional
import time
from datasets import load_dataset
from PIL import Image
from transformers import AutoModelForVision2Seq, AutoProcessor

# ============================================================================
# å¯è°ƒè¶…å‚æ•°é…ç½®åŒºåŸŸ
# ============================================================================

# GPUé…ç½®
GPU_ID = 0  # ä½¿ç”¨å“ªå—GPUï¼ˆ0, 1, 2, ...ï¼‰
DEVICE = f"cuda:{GPU_ID}"

# æ•°æ®é›†é…ç½®
VIDEO_DIR = "/mnt/ssd_ext/huggingface/egoschema/videos"  # è§†é¢‘æ–‡ä»¶å¤¹
DATASET_NAME = "lmms-lab/egoschema"
DATASET_SPLIT = "test"  # 'test' or 'validation'

# æ¨¡å‹é…ç½®
MODEL_PATH = "Qwen/Qwen2.5-VL-7B-Instruct"  # Qwen2.5-VLæ¨¡å‹è·¯å¾„

# è§†é¢‘å¤„ç†é…ç½®
MAX_FRAMES = 360  # æœ€å¤§å¸§æ•°ï¼ˆä¸PruneVidç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰
VIDEO_MIN_PIXELS = 128 * 32 * 32  # è§†é¢‘æœ€å°åƒç´ ï¼ˆ131,072åƒç´ ï¼‰
VIDEO_MAX_PIXELS = 768 * 32 * 32  # è§†é¢‘æœ€å¤§åƒç´ ï¼ˆ786,432åƒç´ ï¼‰

# ç”Ÿæˆé…ç½®
MAX_NEW_TOKENS = 128  # æœ€å¤§ç”Ÿæˆtokenæ•°
TEMPERATURE = 0.0  # 0è¡¨ç¤ºgreedy decoding
DO_SAMPLE = False  # ç¡®å®šæ€§ç”Ÿæˆ

# æµ‹è¯•é…ç½®
NUM_SAMPLES = 500  # æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
START_INDEX = 0  # ä»ç¬¬å‡ ä¸ªæ ·æœ¬å¼€å§‹
SAVE_RESULTS = True  # æ˜¯å¦ä¿å­˜ç»“æœ
OUTPUT_DIR = "./results"  # ç»“æœä¿å­˜ç›®å½•
VERBOSE = True  # æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

# ============================================================================
# ä»£ç å¼€å§‹
# ============================================================================

# è®¾ç½®CUDAè®¾å¤‡
if torch.cuda.is_available():
    torch.cuda.set_device(GPU_ID)


class EgoSchemaBaselineEvaluator:
    """EgoSchemaåŸºçº¿è¯„ä¼°å™¨ - ä½¿ç”¨åŸå§‹Qwen2.5-VLæ¨¡å‹"""

    def __init__(
        self,
        model_path: str,
        video_dir: str,
        dataset_name: str = "lmms-lab/egoschema",
        dataset_split: str = "test",
        device: str = "cuda",
    ):
        self.model_path = model_path
        self.video_dir = Path(video_dir)
        self.dataset_name = dataset_name
        self.dataset_split = dataset_split
        self.device = device

        # åŠ è½½æ¨¡å‹å’Œprocessor
        self.load_model()

        # åŠ è½½æ•°æ®é›†
        self.load_dataset()

        # ç»Ÿè®¡ä¿¡æ¯
        self.total_samples = 0
        self.correct_samples = 0
        self.total_input_tokens = 0
        self.total_output_tokens = 0
        self.results = []

    def load_model(self):
        """åŠ è½½åŸå§‹Qwen2.5-VLæ¨¡å‹"""
        print(f"Loading model: {self.model_path}")
        print("This is the BASELINE version using original transformers implementation")

        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForVision2Seq.from_pretrained(
            self.model_path,
            torch_dtype=torch.bfloat16,
            device_map=self.device if self.device != "cpu" else None,
            trust_remote_code=True,
            attn_implementation="flash_attention_2",  # ä½¿ç”¨FlashAttention2
        ).eval()

        # åŠ è½½processor
        self.processor = AutoProcessor.from_pretrained(
            self.model_path,
            trust_remote_code=True,
            min_pixels=VIDEO_MIN_PIXELS,
            max_pixels=VIDEO_MAX_PIXELS,
        )

        print("Model loaded successfully!\n")

    def load_dataset(self):
        """åŠ è½½EgoSchemaæ•°æ®é›†"""
        print(f"Loading dataset {self.dataset_name} ({self.dataset_split})...")
        self.dataset = load_dataset(self.dataset_name, "Subset", split=self.dataset_split)
        print(f"Loaded {len(self.dataset)} questions\n")

    def load_video(self, video_path: str, max_frames: int = MAX_FRAMES) -> List[Image.Image]:
        """
        åŠ è½½è§†é¢‘å¸§

        Args:
            video_path: è§†é¢‘è·¯å¾„
            max_frames: æœ€å¤§å¸§æ•°

        Returns:
            frames: PIL Imageåˆ—è¡¨
        """
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")

        # è·å–è§†é¢‘ä¿¡æ¯
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)

        if VERBOSE:
            print(f"  Video info: {total_frames} frames, {fps:.2f} FPS")

        # å‡åŒ€é‡‡æ ·
        if total_frames <= max_frames:
            frame_indices = list(range(total_frames))
        else:
            frame_indices = np.linspace(0, total_frames - 1, max_frames, dtype=int).tolist()

        # è¯»å–å¸§
        frames = []
        for idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
            ret, frame = cap.read()
            if ret:
                # BGR to RGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                # è½¬ä¸ºPIL Image
                pil_image = Image.fromarray(frame_rgb)
                frames.append(pil_image)

        cap.release()

        if VERBOSE:
            print(f"  Loaded {len(frames)} frames")

        return frames

    def format_question(self, sample: Dict) -> str:
        """
        æ ¼å¼åŒ–é—®é¢˜ä¸ºé€‰æ‹©é¢˜å½¢å¼

        Args:
            sample: æ ‡æ³¨æ ·æœ¬

        Returns:
            formatted_question: æ ¼å¼åŒ–åçš„é—®é¢˜
        """
        question = sample['question']
        options = sample['option']

        formatted = f"{question}\n\n"
        formatted += "Options:\n"
        for i, opt in enumerate(options):
            formatted += f"{i}. {opt}\n"
        formatted += "\nAnswer with the option number (0-4):"

        return formatted

    def extract_answer(self, generated_text: str) -> Optional[int]:
        """
        ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–ç­”æ¡ˆé€‰é¡¹

        Args:
            generated_text: æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬

        Returns:
            answer: ç­”æ¡ˆé€‰é¡¹ (0-4)ï¼Œå¦‚æœæ— æ³•æå–åˆ™è¿”å›None
        """
        # æ¸…ç†æ–‡æœ¬
        text = generated_text.strip().lower()

        # å°è¯•å¤šç§æå–æ–¹å¼
        # 1. ç›´æ¥æ˜¯æ•°å­—
        if text in ['0', '1', '2', '3', '4']:
            return int(text)

        # 2. å­—æ¯å½¢å¼ (A=0, B=1, C=2, D=3, E=4)
        if text.startswith('a') or text.startswith('option a'):
            return 0
        elif text.startswith('b') or text.startswith('option b'):
            return 1
        elif text.startswith('c') or text.startswith('option c'):
            return 2
        elif text.startswith('d') or text.startswith('option d'):
            return 3
        elif text.startswith('e') or text.startswith('option e'):
            return 4

        # 3. åŒ…å«"option X"æˆ–"é€‰é¡¹ X"
        for i in range(5):
            if f"option {i}" in text or f"é€‰é¡¹ {i}" in text or f"option{i}" in text:
                return i

        # 4. æŸ¥æ‰¾å­—æ¯é€‰é¡¹
        letter_map = {'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4}
        for char in text:
            if char in letter_map:
                return letter_map[char]

        # 5. æŸ¥æ‰¾æ•°å­—0-4
        for char in text:
            if char in '01234':
                return int(char)

        # 6. æ— æ³•æå–
        return None

    def evaluate_sample(self, sample: Dict, sample_idx: int) -> Optional[Dict]:
        """
        è¯„ä¼°å•ä¸ªæ ·æœ¬

        Args:
            sample: æ ‡æ³¨æ ·æœ¬
            sample_idx: æ ·æœ¬ç´¢å¼•

        Returns:
            result: è¯„ä¼°ç»“æœ
        """
        # è·å–è§†é¢‘è·¯å¾„
        video_id = sample['video_idx']
        video_path = self.video_dir / f"{video_id}.mp4"

        if not video_path.exists():
            print(f"Warning: Video not found: {video_path}")
            return None

        # æ ¼å¼åŒ–é—®é¢˜
        question = self.format_question(sample)

        try:
            # åŠ è½½è§†é¢‘
            video_frames = self.load_video(str(video_path), max_frames=MAX_FRAMES)

            # æ„é€ messagesï¼ˆQwen2.5-VLæ ¼å¼ï¼‰
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "video",
                            "video": video_frames,
                            "min_pixels": VIDEO_MIN_PIXELS,
                            "max_pixels": VIDEO_MAX_PIXELS,
                            "total_pixels": VIDEO_MAX_PIXELS,
                        },
                        {"type": "text", "text": question},
                    ],
                }
            ]

            # ä½¿ç”¨processorå¤„ç†è¾“å…¥
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )

            inputs = self.processor(
                text=[text],
                images=None,
                videos=[video_frames],
                padding=True,
                return_tensors="pt",
            ).to(self.device, dtype=torch.bfloat16)

            input_length = inputs['input_ids'].shape[1]
            if VERBOSE:
                print(f"  Input tokens: {input_length}")

            # ç”Ÿæˆ
            with torch.no_grad():
                output_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=MAX_NEW_TOKENS,
                    temperature=TEMPERATURE,
                    do_sample=DO_SAMPLE,
                )

            # è§£ç ï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
            generated_ids = output_ids[:, input_length:]
            generated_text = self.processor.batch_decode(
                generated_ids,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0]

            output_length = generated_ids.shape[1]

            # æå–ç­”æ¡ˆ
            predicted_answer = self.extract_answer(generated_text)
            ground_truth = int(sample['answer'])

            # åˆ¤æ–­æ­£ç¡®æ€§
            is_correct = (predicted_answer == ground_truth)

            return {
                'sample_idx': sample_idx,
                'video_id': video_id,
                'question': sample['question'],
                'ground_truth': ground_truth,
                'predicted_answer': predicted_answer,
                'generated_text': generated_text,
                'is_correct': is_correct,
                'input_tokens': input_length,
                'output_tokens': output_length,
            }

        except Exception as e:
            print(f"Error processing sample {sample_idx}: {e}")
            import traceback
            traceback.print_exc()
            return None

    def run_evaluation(
        self,
        num_samples: Optional[int] = None,
        start_index: int = 0,
    ):
        """
        è¿è¡Œè¯„ä¼°

        Args:
            num_samples: è¯„ä¼°æ ·æœ¬æ•°é‡ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
            start_index: èµ·å§‹ç´¢å¼•
        """
        # ç¡®å®šè¦è¯„ä¼°çš„æ ·æœ¬
        end_index = len(self.dataset) if num_samples is None else min(start_index + num_samples, len(self.dataset))
        samples_to_eval = self.dataset.select(range(start_index, end_index))

        print(f"\n{'='*80}")
        print(f"Starting BASELINE evaluation on {len(samples_to_eval)} samples (from index {start_index})")
        print(f"Model: {self.model_path}")
        print(f"{'='*80}\n")

        # è¯„ä¼°æ¯ä¸ªæ ·æœ¬
        start_time = time.time()

        for i, sample in enumerate(samples_to_eval):
            sample_idx = start_index + i

            if VERBOSE:
                print(f"Sample {i + 1}/{len(samples_to_eval)}")
                print(f"Video ID: {sample['video_idx']}")
                print(f"Question: {sample['question'][:100]}...")

            # è¯„ä¼°æ ·æœ¬
            result = self.evaluate_sample(sample, sample_idx)

            if result is None:
                continue

            # æ›´æ–°ç»Ÿè®¡
            self.total_samples += 1
            if result['is_correct']:
                self.correct_samples += 1

            self.total_input_tokens += result['input_tokens']
            self.total_output_tokens += result['output_tokens']
            self.results.append(result)

            # æ‰“å°ç»“æœ
            if VERBOSE:
                print(f"\nGround Truth: {result['ground_truth']}")
                print(f"Predicted:    {result['predicted_answer']}")
                print(f"Generated:    '{result['generated_text']}'")
                print(f"Correct: {'âœ“' if result['is_correct'] else 'âœ—'}")

                # å½“å‰ç´¯è®¡å‡†ç¡®ç‡
                accuracy = self.correct_samples / self.total_samples * 100
                print(f"\nğŸ“Š Current Accuracy: {self.correct_samples}/{self.total_samples} = {accuracy:.2f}%")

                # å¹³å‡tokenæ•°é‡
                avg_input = self.total_input_tokens / self.total_samples
                avg_output = self.total_output_tokens / self.total_samples
                print(f"ğŸ“Š Average tokens - Input: {avg_input:.0f}, Output: {avg_output:.0f}")
                print(f"\n{'='*80}\n")

        total_time = time.time() - start_time

        # æ‰“å°æœ€ç»ˆç»“æœ
        self.print_final_results(total_time)

    def print_final_results(self, total_time: float):
        """æ‰“å°æœ€ç»ˆè¯„ä¼°ç»“æœ"""
        print(f"\n{'='*80}")
        print(f"ğŸ‰ BASELINE EVALUATION COMPLETED")
        print(f"{'='*80}\n")

        # å‡†ç¡®ç‡
        accuracy = self.correct_samples / self.total_samples * 100 if self.total_samples > 0 else 0
        print(f"ğŸ“Š Final Accuracy:")
        print(f"  Correct: {self.correct_samples}/{self.total_samples}")
        print(f"  Accuracy: {accuracy:.2f}%")

        # Tokenç»Ÿè®¡
        avg_input = self.total_input_tokens / self.total_samples if self.total_samples > 0 else 0
        avg_output = self.total_output_tokens / self.total_samples if self.total_samples > 0 else 0
        print(f"\nğŸ“Š Token Statistics:")
        print(f"  Total input tokens:  {self.total_input_tokens}")
        print(f"  Total output tokens: {self.total_output_tokens}")
        print(f"  Avg input tokens:    {avg_input:.1f}")
        print(f"  Avg output tokens:   {avg_output:.1f}")

        # æ—¶é—´ç»Ÿè®¡
        avg_time_per_sample = total_time / self.total_samples if self.total_samples > 0 else 0
        print(f"\nâ±ï¸  Time Statistics:")
        print(f"  Total time: {total_time:.2f}s")
        print(f"  Avg time per sample: {avg_time_per_sample:.2f}s")

        print(f"\n{'='*80}\n")

        # ä¿å­˜ç»“æœ
        if SAVE_RESULTS:
            self.save_results()

    def save_results(self):
        """ä¿å­˜è¯„ä¼°ç»“æœ"""
        os.makedirs(OUTPUT_DIR, exist_ok=True)

        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f"egoschema_results_{timestamp}_baseline.json"
        filepath = os.path.join(OUTPUT_DIR, filename)

        # å‡†å¤‡ä¿å­˜çš„æ•°æ®
        save_data = {
            'config': {
                'model_path': self.model_path,
                'model_type': 'baseline',
                'max_frames': MAX_FRAMES,
                'video_min_pixels': VIDEO_MIN_PIXELS,
                'video_max_pixels': VIDEO_MAX_PIXELS,
                'num_samples': NUM_SAMPLES,
                'dataset': self.dataset_name,
                'split': self.dataset_split,
            },
            'summary': {
                'total_samples': self.total_samples,
                'correct_samples': self.correct_samples,
                'accuracy': self.correct_samples / self.total_samples * 100 if self.total_samples > 0 else 0,
                'total_input_tokens': self.total_input_tokens,
                'total_output_tokens': self.total_output_tokens,
                'avg_input_tokens': self.total_input_tokens / self.total_samples if self.total_samples > 0 else 0,
                'avg_output_tokens': self.total_output_tokens / self.total_samples if self.total_samples > 0 else 0,
            },
            'results': self.results,
        }

        # ä¿å­˜
        with open(filepath, 'w') as f:
            json.dump(save_data, f, indent=2)

        print(f"âœ… Results saved to: {filepath}")


def main():
    """ä¸»å‡½æ•°"""
    print(f"\n{'='*80}")
    print(f"EgoSchema Evaluation - Qwen2.5-VL Baseline (No PruneVid)")
    print(f"{'='*80}\n")

    print(f"Configuration:")
    print(f"  GPU: {GPU_ID} (device: {DEVICE})")
    print(f"  Model: {MODEL_PATH}")
    print(f"  Dataset: {DATASET_NAME} ({DATASET_SPLIT})")
    print(f"  Video dir: {VIDEO_DIR}")
    print(f"\nVideo Settings:")
    print(f"  Max frames: {MAX_FRAMES}")
    print(f"  Video min pixels: {VIDEO_MIN_PIXELS:,}")
    print(f"  Video max pixels: {VIDEO_MAX_PIXELS:,}")
    print(f"\nTest Settings:")
    print(f"  Num samples: {NUM_SAMPLES if NUM_SAMPLES else 'All'}")
    print(f"  Start index: {START_INDEX}")
    print(f"  Temperature: {TEMPERATURE}")
    print(f"  Do sample: {DO_SAMPLE}")
    print(f"\n{'='*80}\n")

    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = EgoSchemaBaselineEvaluator(
        model_path=MODEL_PATH,
        video_dir=VIDEO_DIR,
        dataset_name=DATASET_NAME,
        dataset_split=DATASET_SPLIT,
        device=DEVICE,
    )

    # è¿è¡Œè¯„ä¼°
    evaluator.run_evaluation(
        num_samples=NUM_SAMPLES,
        start_index=START_INDEX,
    )


if __name__ == "__main__":
    main()

"""
PruneVidé›†æˆQwen2.5-VLæ¨¡å‹
===========================

è¿™ä¸ªæ¨¡å—é€šè¿‡wrapperæ–¹å¼å°†PruneVidçš„3ä¸ªé˜¶æ®µé›†æˆåˆ°Qwen2.5-VLæ¨¡å‹ä¸­ã€‚

è®¾è®¡æ€è·¯ï¼š
1. ä¸ç›´æ¥ä¿®æ”¹transformersçš„Qwen2.5-VLæºç 
2. é€šè¿‡wrapperç±»åŒ…è£…åŸå§‹æ¨¡å‹
3. åœ¨å…³é”®ä½ç½®æ’å…¥PruneVidçš„3ä¸ªstage
4. ä¿æŒä¸transformers APIçš„å…¼å®¹æ€§

é›†æˆç‚¹ï¼š
- Stage 1: åœ¨vision encoderä¹‹åï¼ŒLLMå¤„ç†ä¹‹å‰
- Stage 2: åœ¨LLMçš„ç¬¬Må±‚é€šè¿‡hookæå–æ³¨æ„åŠ›
- Stage 3: é€šè¿‡è‡ªå®šä¹‰cacheåœ¨generateæ—¶ç”Ÿæ•ˆ

æ³¨æ„äº‹é¡¹ï¼š
ç”±äºQwen2.5-VLçš„æ¶æ„ç‰¹æ®Šæ€§ï¼ˆvision encoderé›†æˆåœ¨æ¨¡å‹å†…éƒ¨ï¼‰ï¼Œ
æˆ‘ä»¬éœ€è¦åœ¨åˆé€‚çš„ä½ç½®æ‹¦æˆªå’Œä¿®æ”¹tokenæµã€‚
"""

import torch
import torch.nn as nn
from typing import Optional, Tuple, Dict, List, Union
from transformers import AutoModelForVision2Seq, AutoProcessor
from transformers.modeling_outputs import CausalLMOutputWithPast

from config import PruneVidConfig
from stage1_temporal_spatial_merge import SpatialTemporalTokenMerger
from stage2_attention_selection import AttentionBasedTokenSelector
from stage3_kv_cache import PruneVidDynamicCache


class Qwen2VLForConditionalGenerationWithPruneVid(nn.Module):
    """
    é›†æˆPruneVidçš„Qwen2.5-VLæ¨¡å‹

    è¿™ä¸ªç±»åŒ…è£…åŸå§‹çš„Qwen2VLForConditionalGenerationï¼Œ
    åœ¨forwardå’Œgenerateæµç¨‹ä¸­é›†æˆPruneVidçš„3ä¸ªstageã€‚
    """

    def __init__(
        self,
        base_model: nn.Module,
        config: PruneVidConfig,
    ):
        """
        åˆå§‹åŒ–

        Args:
            base_model: é¢„è®­ç»ƒçš„Qwen2.5-VLæ¨¡å‹
            config: PruneVidé…ç½®
        """
        super().__init__()

        self.base_model = base_model
        self.config = config

        # åˆå§‹åŒ–3ä¸ªstage
        self.stage1 = SpatialTemporalTokenMerger(config) if config.enable_stage1 else None
        self.stage2 = AttentionBasedTokenSelector(config) if config.enable_stage2 else None

        # Stage 3åœ¨generateæ—¶é€šè¿‡past_key_valueså‚æ•°ä¼ å…¥

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {}

        # ç‰¹æ®Štoken IDsï¼ˆç”¨äºæ£€æµ‹è§†è§‰tokenä½ç½®ï¼‰
        # Qwen2.5-VLä½¿ç”¨ç‰¹æ®Štokenæ ‡è®°è§†è§‰å†…å®¹çš„å¼€å§‹å’Œç»“æŸ
        self.vision_start_token_id = getattr(base_model.config, 'vision_start_token_id', None)
        self.vision_end_token_id = getattr(base_model.config, 'vision_end_token_id', None)

        if self.config.verbose:
            print(f"[PruneVid] åˆå§‹åŒ–å®Œæˆ")
            print(f"  Stage 1: {'å¯ç”¨' if config.enable_stage1 else 'ç¦ç”¨'}")
            print(f"  Stage 2: {'å¯ç”¨' if config.enable_stage2 else 'ç¦ç”¨'}")
            print(f"  Stage 3: {'å¯ç”¨' if config.enable_cache_compression else 'ç¦ç”¨'}")

    def _detect_visual_tokens(self, input_ids: torch.Tensor) -> Tuple[Optional[int], Optional[int]]:
        """
        æ£€æµ‹è§†è§‰tokençš„ä½ç½®

        Qwen2.5-VLä½¿ç”¨ç‰¹æ®Štokenæ¥æ ‡è®°è§†è§‰å†…å®¹ï¼š
        [text...] <vision_start> [visual tokens...] <vision_end> [text...]

        Args:
            input_ids: [batch, seq_len] è¾“å…¥token IDs

        Returns:
            visual_start: è§†è§‰tokenèµ·å§‹ä½ç½®ï¼ˆvision_startä¹‹åï¼‰
            visual_end: è§†è§‰tokenç»“æŸä½ç½®ï¼ˆvision_endä¹‹å‰ï¼‰
            å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°è§†è§‰tokenï¼Œè¿”å›(None, None)
        """
        if self.vision_start_token_id is None or self.vision_end_token_id is None:
            # å¦‚æœæ²¡æœ‰é…ç½®ç‰¹æ®Štokenï¼Œå°è¯•å¸¸è§çš„ID
            # è¿™æ˜¯ä¸€ä¸ªfallbackï¼Œå®é™…ä½¿ç”¨ä¸­åº”è¯¥ä»configè·å–
            if self.config.verbose:
                print("[PruneVid] è­¦å‘Šï¼šæœªæ‰¾åˆ°vision token IDsï¼Œå°†å°è¯•å¯å‘å¼æ£€æµ‹")
            return None, None

        batch_size = input_ids.shape[0]
        if batch_size != 1:
            # ç›®å‰åªæ”¯æŒbatch=1
            if self.config.verbose:
                print(f"[PruneVid] è­¦å‘Šï¼šbatch_size={batch_size} > 1ï¼Œæš‚ä¸æ”¯æŒ")
            return None, None

        input_ids_single = input_ids[0]  # [seq_len]

        # æŸ¥æ‰¾vision_startå’Œvision_end
        start_positions = (input_ids_single == self.vision_start_token_id).nonzero(as_tuple=True)[0]
        end_positions = (input_ids_single == self.vision_end_token_id).nonzero(as_tuple=True)[0]

        if len(start_positions) == 0 or len(end_positions) == 0:
            # æ²¡æœ‰æ‰¾åˆ°è§†è§‰token
            return None, None

        # å–ç¬¬ä¸€å¯¹
        visual_start = start_positions[0].item() + 1  # +1å› ä¸ºè¦è·³è¿‡start tokenæœ¬èº«
        visual_end = end_positions[0].item()  # end tokenä¹‹å‰

        if visual_start >= visual_end:
            if self.config.verbose:
                print(f"[PruneVid] è­¦å‘Šï¼šæ— æ•ˆçš„è§†è§‰tokenèŒƒå›´ [{visual_start}, {visual_end})")
            return None, None

        if self.config.verbose:
            print(f"[PruneVid] æ£€æµ‹åˆ°è§†è§‰tokens: [{visual_start}, {visual_end}), æ•°é‡={visual_end - visual_start}")

        return visual_start, visual_end

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.Tensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        return_dict: Optional[bool] = None,
        **kwargs
    ) -> Union[Tuple, CausalLMOutputWithPast]:
        """
        Forwardæ–¹æ³• - é›†æˆPruneVid

        æµç¨‹ï¼š
        1. è°ƒç”¨åŸºç¡€æ¨¡å‹çš„embeddingå±‚
        2. æ£€æµ‹è§†è§‰tokenä½ç½®
        3. Stage 1: æ—¶ç©ºtokenåˆå¹¶
        4. Stage 2: è®¾ç½®attention hookï¼ˆåœ¨æŒ‡å®šå±‚æå–æ³¨æ„åŠ›ï¼‰
        5. è°ƒç”¨åŸºç¡€æ¨¡å‹çš„ä¸»ä½“ï¼ˆä¼šè§¦å‘Stage 2ï¼‰
        6. è¿”å›ç»“æœ

        æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç®€åŒ–å®ç°ã€‚å®Œæ•´å®ç°éœ€è¦æ·±å…¥ä¿®æ”¹forwardæµç¨‹ã€‚
        """
        # ç®€åŒ–å®ç°ï¼šç›´æ¥è°ƒç”¨åŸºç¡€æ¨¡å‹
        # å®Œæ•´å®ç°éœ€è¦æ‹¦æˆªä¸­é—´å±‚ï¼Œè¿™é‡Œæˆ‘ä»¬é€šè¿‡generateæ–¹æ³•æ¥å®ç°ä¸»è¦åŠŸèƒ½

        # å¯¹äºtraining/evaluationï¼Œæš‚æ—¶ä¸å¯ç”¨PruneVidï¼ˆéœ€è¦æ›´å¤æ‚çš„é›†æˆï¼‰
        if labels is not None:
            if self.config.verbose:
                print("[PruneVid] è®­ç»ƒæ¨¡å¼ï¼Œç¦ç”¨PruneVid")
            return self.base_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                position_ids=position_ids,
                past_key_values=past_key_values,
                inputs_embeds=inputs_embeds,
                labels=labels,
                use_cache=use_cache,
                output_attentions=output_attentions,
                output_hidden_states=output_hidden_states,
                return_dict=return_dict,
                **kwargs
            )

        # å¯¹äºinferenceï¼Œä¹Ÿå…ˆè°ƒç”¨åŸºç¡€æ¨¡å‹
        # Stage 2éœ€è¦åœ¨ç‰¹å®šå±‚æå–æ³¨æ„åŠ›ï¼Œè¿™éœ€è¦hook
        if self.config.enable_stage2 and output_attentions is None:
            output_attentions = True

        outputs = self.base_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            labels=labels,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            **kwargs
        )

        return outputs

    @torch.no_grad()
    def generate(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        **generate_kwargs
    ) -> torch.LongTensor:
        """
        ç”Ÿæˆæ–¹æ³• - å®Œæ•´é›†æˆPruneVidçš„3ä¸ªstage

        è¿™æ˜¯PruneVidä¸»è¦ç”Ÿæ•ˆçš„åœ°æ–¹ã€‚

        æµç¨‹ï¼š
        1. Prefillé˜¶æ®µï¼š
           a. æ£€æµ‹è§†è§‰tokenä½ç½®
           b. Stage 1: åˆå¹¶è§†è§‰tokenï¼ˆéœ€è¦åœ¨embeddingåæ‹¦æˆªï¼‰
           c. Stage 2: åœ¨ç¬¬Må±‚æå–æ³¨æ„åŠ›å¹¶é€‰æ‹©token
        2. Decodeé˜¶æ®µï¼š
           a. Stage 3: ä½¿ç”¨å‹ç¼©çš„KV cache

        æ³¨æ„ï¼šå®Œæ•´å®ç°Stage 1éœ€è¦ä¿®æ”¹æ¨¡å‹å†…éƒ¨çš„forwardæµç¨‹ã€‚
        å½“å‰ç‰ˆæœ¬é€šè¿‡processoré¢„å¤„ç†æ¥è¿‘ä¼¼å®ç°ã€‚
        """
        # æ£€æµ‹è§†è§‰token
        visual_start, visual_end = self._detect_visual_tokens(input_ids)

        # å‡†å¤‡past_key_valuesï¼ˆStage 3ï¼‰
        if self.config.enable_cache_compression:
            past_key_values = PruneVidDynamicCache(verbose=self.config.verbose)
        else:
            past_key_values = None

        # è®¾ç½®Stage 2çš„hook
        if self.config.enable_stage2 and visual_start is not None:
            target_layer = self.base_model.model.language_model.layers[self.config.pruning_layer]
            self.stage2.setup_hook(target_layer, visual_start, visual_end)

            if self.config.verbose:
                print(f"[PruneVid] Stage 2 hookå·²è®¾ç½®åœ¨layer {self.config.pruning_layer}")

        # è°ƒç”¨åŸºç¡€æ¨¡å‹çš„generate
        # æ³¨æ„ï¼šä¸ºäº†è®©attention hookå·¥ä½œï¼Œéœ€è¦output_attentions=Trueï¼ˆåœ¨prefillé˜¶æ®µï¼‰
        if 'output_attentions' not in generate_kwargs and self.config.enable_stage2:
            generate_kwargs['output_attentions'] = True

        # ä¼ é€’è‡ªå®šä¹‰cache
        if past_key_values is not None:
            generate_kwargs['past_key_values'] = past_key_values

        outputs = self.base_model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            **generate_kwargs
        )

        # æ¸…ç†hook
        if self.config.enable_stage2:
            self.stage2.remove_hook()

        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        if self.config.collect_stats:
            stats = {}
            if self.stage1 is not None:
                stats['stage1'] = self.stage1.last_stats
            if self.stage2 is not None:
                stats['stage2'] = self.stage2.last_stats
            if isinstance(past_key_values, PruneVidDynamicCache):
                stats['stage3'] = past_key_values.get_compression_stats()
            self.stats = stats

        return outputs

    def get_stats(self) -> Dict:
        """è·å–æœ€è¿‘ä¸€æ¬¡æ¨ç†çš„ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats

    @property
    def device(self):
        """è·å–æ¨¡å‹è®¾å¤‡"""
        return self.base_model.device

    def to(self, *args, **kwargs):
        """ç§»åŠ¨æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡"""
        self.base_model = self.base_model.to(*args, **kwargs)
        return self

    def eval(self):
        """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
        self.base_model.eval()
        return self

    def train(self, mode: bool = True):
        """è®¾ç½®è®­ç»ƒæ¨¡å¼"""
        self.base_model.train(mode)
        return self


def load_prunevid_model(
    model_name_or_path: str,
    config: Optional[PruneVidConfig] = None,
    device: str = "cuda",
    torch_dtype: torch.dtype = torch.bfloat16,
):
    """
    åŠ è½½é›†æˆPruneVidçš„Qwen2.5-VLæ¨¡å‹ï¼ˆä½¿ç”¨DTD-basedå®ç°ï¼‰

    Args:
        model_name_or_path: æ¨¡å‹è·¯å¾„æˆ–HuggingFace ID
        config: PruneVidé…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨baselineï¼ˆä¸å‰ªæï¼‰
        device: è®¾å¤‡
        torch_dtype: æ•°æ®ç±»å‹

    Returns:
        model: é›†æˆPruneVidçš„æ¨¡å‹
        processor: å¯¹åº”çš„processor
    """
    from config import get_baseline_config
    from modeling_qwen2_5_vl_prunevid_dtd import Qwen2_5_VLForConditionalGeneration
    from configuration_qwen2_5_vl import Qwen2_5_VLConfig

    if config is None:
        config = get_baseline_config()

    # ç›´æ¥ä½¿ç”¨transformersåŠ è½½ï¼Œç„¶ååŒ…è£…
    print(f"åŠ è½½Qwen2.5-VLæ¨¡å‹: {model_name_or_path}")

    # å…ˆç›´æ¥åŠ è½½é¢„è®­ç»ƒæ¨¡å‹åˆ°ç›®æ ‡è®¾å¤‡
    from transformers import AutoModelForVision2Seq
    pretrained_model = AutoModelForVision2Seq.from_pretrained(
        model_name_or_path,
        torch_dtype=torch_dtype,
        device_map=device if device != "cpu" else None,
        trust_remote_code=True,
        attn_implementation="flash_attention_2",  # ä½¿ç”¨FlashAttention2 (æœ€ä¼˜æ€§èƒ½)
    )

    # åˆ›å»ºæˆ‘ä»¬çš„PruneVidæ¨¡å‹ï¼Œä½¿ç”¨ç›¸åŒçš„config
    model = Qwen2_5_VLForConditionalGeneration(pretrained_model.config, prunevid_config=config)

    # ğŸ”§ å…³é”®ä¿®å¤ï¼šè½¬æ¢æƒé‡keysä»¥åŒ¹é…PruneVidçš„æ¨¡å‹ç»“æ„
    # transformersçš„åŸå§‹ç»“æ„: model.visual.*, model.language_model.*
    # PruneVidçš„ç»“æ„: visual.*, model.*
    def convert_state_dict_keys(state_dict):
        """è½¬æ¢transformersçš„state_dict keysåˆ°PruneVidæ ¼å¼"""
        new_state_dict = {}
        for key, value in state_dict.items():
            # è½¬æ¢visualéƒ¨åˆ†: model.visual.* -> visual.*
            if key.startswith('model.visual.'):
                new_key = key.replace('model.visual.', 'visual.')
            # è½¬æ¢language modeléƒ¨åˆ†: model.language_model.* -> model.*
            elif key.startswith('model.language_model.'):
                new_key = key.replace('model.language_model.', 'model.')
            # å…¶ä»–ä¿æŒä¸å˜
            else:
                new_key = key
            new_state_dict[new_key] = value
        return new_state_dict

    # è½¬æ¢åŸå§‹æ¨¡å‹çš„state_dict
    print("[PruneVid] Converting state_dict keys to match PruneVid structure...")
    converted_state_dict = convert_state_dict_keys(pretrained_model.state_dict())

    # å¤åˆ¶æƒé‡ï¼ˆpretrained_modelå·²ç»åœ¨ç›®æ ‡è®¾å¤‡ä¸Šï¼‰
    # ä½¿ç”¨strict=Trueç¡®ä¿æƒé‡å®Œå…¨åŒ¹é…ï¼Œé¿å…é™é»˜çš„æƒé‡åŠ è½½é”™è¯¯
    try:
        model.load_state_dict(converted_state_dict, strict=True)
        print("[PruneVid] âœ… Successfully loaded all weights with strict=True")
    except RuntimeError as e:
        print(f"[PruneVid WARNING] Failed to load weights with strict=True: {e}")
        print("[PruneVid] Falling back to strict=False...")
        missing_keys, unexpected_keys = model.load_state_dict(converted_state_dict, strict=False)
        if missing_keys:
            print(f"[PruneVid WARNING] Missing keys ({len(missing_keys)}): {missing_keys[:5]}...")
        if unexpected_keys:
            print(f"[PruneVid WARNING] Unexpected keys ({len(unexpected_keys)}): {unexpected_keys[:5]}...")

    # ç¡®ä¿æ¨¡å‹åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    if device != "cpu":
        model = model.to(device)

    model.eval()

    # åˆ é™¤pretrained_modelé‡Šæ”¾å†…å­˜
    del pretrained_model
    import gc
    gc.collect()
    if device != "cpu":
        torch.cuda.empty_cache()

    # åŠ è½½processor
    processor = AutoProcessor.from_pretrained(
        model_name_or_path,
        trust_remote_code=True,
    )

    print(f"PruneVidæ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"  é…ç½®: {config.to_dict()}")

    return model, processor


# å¯¼å‡º
__all__ = [
    "Qwen2VLForConditionalGenerationWithPruneVid",
    "load_prunevid_model",
]
